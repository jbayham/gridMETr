---
title: "A Weighted Average by Population and Area"
subtitle: "Weather from a netCDF"
author: "Gal Koss and Jude Bayham"
date: "03/23/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(raster)
library(sf)
library(knitr)
library(mapview)
library(ggmap)
library(ggthemes)
library(kableExtra)
library(data.table)
library(ncdf4)
knitr::opts_chunk$set(echo = T,
                      message = F,
                      fig.align = "center")
```
<br>
<br>

## Before You Start

<br>
As a demonstration for using R as GIS, in this section we describe a method for extracting data from a netCDF, using [gridMET](http://www.climatologylab.org/gridmet.html) data as an example. In this instance, we use spatially and temporally explicit precipitation data to understand the impact of rain on fatal car crashes.
<br>
<br>
Generally, when extracting data from a netCDF intended to describe the climate in a polygon (e.g. a county), it is worthwhile computationally (both in operating and storage) to aggregate the data as you extracts it. The innovation that we describe allows you to aggregate the data for any polygon, with weights for the population in each gridMET cell (using [LandScan](https://landscan.ornl.gov) raster data) and the portion of the gridMET cell within the polygon of interest. Weighting by area allows you to accurately consider the climate variable values in cells only partially contained within the polygon. Weighting by population allows the aggregated estimate to take into account the weather experienced by people in the polygon, as opposed to an unweighted aggregation that would consider the rain in an empty field as impactful as the rain in a  populated residential neighborhood. It is important to weight by population in this case since the model relies on weather's impacts on people and we assume that both population and weather are spatially heterogeneous within a polygon. 
<br>
<br>
We will start by downloading a sample gridMET netCDF, a dataframe of polygons, and population data in [Setting Up]. In [Weighting], we use raster operations to align the cells from LandScan and gridMET, and build a bridge between the gridMET cells and polygons that uses population and area to create a total index for each gridmet cell for each polygon. We extract precipitation values from the netCDF using the bridge in [Extracting], and end up with rainfall by county and day. Finally, we estimate the relationship between rainfall and car accidents in a simple linear regression model in [Regression].
<br>
<br>
<br>

## Setting Up

<br>
Using the R library `downloader`, we can download the gridMET data for precipitation in 2020 fairly easily. This can be looped over ([Appendix A]) with the help of a list of variables, years, and `paste0()`. Our polygons of interest will be counties, which will we access via the library [`tigris`](https://github.com/walkerke/tigris). Finally, we download the corresponding population data from LandScan. This data is available for those within the educational community, and for ease, we access a zipped version from Google Drive using the R library [`googledrive`](https://googledrive.tidyverse.org). 
<br>
<br>
```{r eval = F}
#--- download gridMET precipitation 2020 ---#
downloader::download(url=str_c("http://www.northwestknowledge.net/metdata/data/pr_2018.nc"),
                     destfile = "pr_2018.nc",
                     mode = 'wb')

#--- download TIGER CBSA ---#
counties <- tigris::counties() %>% 
  filter(!str_detect(NAME, ", PR|, HI|, AK")) %>% 
  st_transform(4326)

#--- download Landscan 2020 ---#
googledrive::drive_download(paste0("Landscan/LandScan Global 2018.zip"),
                            path = "Landscan_2018.zip")  
  
unzip("Landscan_2018.zip")
```
<br>
We continue to ingest the gridMET netCDF by applying the `brick()` function from the `raster` library to convert the netCDF to a raster. We only need the coordinates so we use `raster::xyFromCell()` on the first slice (day) in the brick, subsetted accordingly. Using `dplyr`, we create a tibble `slicecoords` that also also has a column `cell` with uniquely identifying cell numbers. 
<br>
<br>
``` {r eval = F}
#--- ingest a slice of the netCDF brick ---#
slice <- raster::brick("pr_2018.nc")[[1]] # can ignore warning

#--- make tibble of coordinates ---#
slicecoords <- tibble(lon = xyFromCell(slice, 1:ncell(slice))[,1],
                      lat = xyFromCell(slice, 1:ncell(slice))[,2]) %>%
  rownames_to_column(var = "cell") %>% 
  mutate(cell = as.numeric(cell))
```
<br>
There are 810,810 rows in our resulting tibble, one for each gridMET cell. While we used a single day from precipitation data, this tibble can be used to describe any gridMET netCDF since they all use the same grid. We continue to ingest the population data using `raster::raster()` on the primary file in the unzipped folder, and then cropping using `raster::crop()` on the resulting raster and the extent of the polygons of interest ([Cropping (Spatial subsetting) to the Area of Interest]). Cropping allows us to restrict the LandScan data from 933M cells (globally) to 77M cells (the US). \par
<br>
```{r eval = F}
#--- ingest ---#
landscan <- raster("LandScan Global 2018/lspop2018/w001001.adf")

#--- crop to the polygon extent ---#
uslandscan <- crop(landscan,extent(counties))
```
<br>
<br>

## Weighting

<br>
With our data ingested, we can begin to align the gridMET and LandScan data in 3 steps. 

1. `terra::resample()` to convert the LandScan raster to the same resolution (extent, origin, and number of rows and columns). They already have the same crs. We use a bilinear interpolation method to take the average of the four nearest pixels.
2. `terra::mosaic()` to reconcile the different cell sizes
3. `terra::extract` to get the raster cells from `together` found in each polygon. The result is a list of data frames for each polygon with the cell numbers from the raster, the corresponding population, and weights for the area of the cell contained in the polygon. If the whole cell is contained within the polygon, its area weight = $1$; if half a cell is within the polygon, its area weight = $0.5$.
<br>
<br>
```{r eval = F}
#--- 1. same resolution as netcdf ---#
uslandscan_resampled <- terra::resample(uslandscan, slice, method = "bilinear") 

#--- 2. reconcile cell values ---#
together <- terra::mosaic(slice,uslandscan_resampled,fun=sum)

#--- 3. match raster cells to polygons ---#
extracted <- terra::extract(together,counties,
                            weights = T, 
                            normalizeWeights = F,
                            cellnumbers = T)
```
<br>
In the below plots of Bend, OR, we can see how the population and area weights are assigned. 

```{r echo=FALSE, message=FALSE, warning=FALSE, results = "hide"}
cbsa1 <- cbsa <- tigris::core_based_statistical_areas() %>% 
  filter(!str_detect(NAME, ", PR|, HI|, AK")) %>% 
  st_transform(4326) %>% 
  slice(36)

bb <- st_bbox(st_buffer(cbsa1,0.2))

gg_bbox <- c(left=bb[[1]],
              bottom=bb[[2]],
              right=bb[[3]],
              top=bb[[4]])

basemap <- get_stamenmap(
  bbox = gg_bbox,
  zoom = 9,
  maptype = "toner")
basemap_attributes <- attributes(basemap)
basemap_transparent <- matrix(adjustcolor(basemap, 
                                           alpha.f = 0.75), 
                               nrow = nrow(basemap))
attributes(basemap_transparent) <- basemap_attributes
# 
# temp <- extracted[[36]] %>%
#   as_tibble() %>%
#   left_join(.,slicecoords,by="cell") %>%
#   mutate(popweight = value*weight,
#          totalindex = popweight/sum(popweight),
#          Population = case_when(value > 500 ~ "500+",
#                                 value > 5 ~ "5-500",
#                                 value > 1 ~ "1-5",
#                                 T ~ "<1"),
#          "Area Weight" = weight,
#          "Total Index" = case_when(totalindex > 0.05 ~ "5%+",
#                                    totalindex > 0.001 ~ "0.1-5%",
#                                    totalindex > 0.0001 ~ "0.01-0.1%",
#                                    T ~ "<0.01%")) %>%
#   st_as_sf(coords = c(x = "lon",y= "lat"), crs = 4269)
# 
# temp$Population <- factor(temp$Population, levels = c("500+",
#                                                       "5-500",
#                                                       "1-5",
#                                                       "<1"))
# temp$`Total Index` <- factor(temp$`Total Index`, levels = c("5%+",
#                                                             "0.1-5%",
#                                                             "0.01-0.1%",
#                                                             "<0.01%"))
# 
# write_rds(temp,"projects/Pop Weights/temp.rds")
temp <- read_rds("temp.rds")

pop <- ggmap(basemap_transparent) +
  geom_sf(data = cbsa1, inherit.aes = F, alpha = 0.5, fill = "lightblue", color = "grey") +
  geom_sf(data = temp, aes(fill = Population, size = `Area Weight`), 
          inherit.aes = F, alpha = 0.5, shape = 22) +
  scale_fill_manual(values = c("#040014","#832232","#CE8964","#EAF27C")) +
  scale_size(range = c(1,3)) +
  ggsn::scalebar(x.min = bb[[1]], x.max = bb[[3]]-0.1,
                 y.min = bb[[2]], y.max = bb[[4]],
                 dist = 10, dist_unit = "km",
                 st.bottom = F, st.size = 3,
                 transform = TRUE, model = "WGS84",
                 location = "bottomright") +
  ggsn::north(x.min = bb[[1]], x.max = bb[[3]],
        y.min = bb[[2]], y.max = bb[[4]],
        location = "topleft",
        symbol = 12, scale = 0.08) +
  labs(title = "Population in Each gridMET Cell") +
  guides(fill = guide_legend(override.aes = list(size = 5))) +
  theme_tufte(ticks = F, base_size = 16) +
  theme(axis.text = element_blank(),
        axis.title = element_blank())

indx <- ggmap(basemap_transparent) +
  geom_sf(data = cbsa1, inherit.aes = F, alpha = 0.5, fill = "lightblue", color = "grey") +
  geom_sf(data = temp, aes(fill = `Total Index`), 
          size = 3, inherit.aes = F, alpha = 0.5, shape = 22) +
  scale_fill_manual(values = c("#040014","#832232","#CE8964","#EAF27C")) +
  ggsn::scalebar(x.min = bb[[1]], x.max = bb[[3]]-0.1,
                 y.min = bb[[2]], y.max = bb[[4]],
                 dist = 10, dist_unit = "km",
                 st.bottom = F, st.size = 3,
                 transform = TRUE, model = "WGS84",
                 location = "bottomright") +
  ggsn::north(x.min = bb[[1]], x.max = bb[[3]],
        y.min = bb[[2]], y.max = bb[[4]],
        location = "topleft",
        symbol = 12, scale = 0.08) +
  labs(title = "Index Value for Each gridMET Cell") +
  theme_tufte(ticks = F, base_size = 16) +
  theme(axis.text = element_blank(),
        axis.title = element_blank())

pop
```

By multiplying the population in a cell by the area weight from `extract`, we can get a population weight. We construct a total index as the portion of area-weighted population in that cell, divided by the sum of all area weighted population. 

```{r echo=FALSE}
indx
```

To calculate this for a single polygon, we use `data.table` functions within a `map2_dfr()`, looping over each polygon as it corresponds to the `extracted` list of dataframes and rows of `cbsa`. For each polygon in the loop, we merge with the netCDF `slicecoords` dataframe (see above). We then create the population weight by multiplying the population from the LandScan with the area of the polygon covered, and the total index by dividing the weighted population in that cell by the total weighted population in all cells found in the polygon. 
<br>
```{r, eval = F}
bridge <- map2_dfr(extracted,counties$GEOID,function(e1,GEOID){
  
  e1dt <- e1 %>% 
      as.data.table()
  
  if (nrow(e1dt) > 0){
    
    temp <- e1dt %>% 
      merge(., slicecoords, all.x=T, by="cell") %>%
      # left_join(., slicecoords, by = "cell") %>% 
      .[, c("popweight") :=
          .(value*weight)] %>% 
      .[, c("totalindex","GEOID") :=
          .(popweight/sum(popweight),GEOID)] %>% 
      dplyr::select(GEOID,lon,lat,totalindex)
    
  } 
})
```

```{r include=FALSE}
# write_rds(bridge,"projects/Pop Weights/bridgecounty.rds")
bridge <- read_rds("bridgecounty.rds")
```
<br>
Note that we use a row binding map (`_dfr`), meaning that our final dataframe has unique identifiers of `GEOID`, `lat`, and `lon`: every row is the imputed index value of a gridMET cell for a polygon. 
<br>
<br>
<br>

## Extracting

<br>
Using the R library `ncdf4`, we extract the data from the brick of 2018 precipitation. We do it in a few steps, 

1. Opening the file
2. Getting the variable name and dates (useful for when looping over several files)
3. Reading in the data, and organizing it into a useful form (matrix, and then dataframe with the coordinates from the earlier `slicecoords`) 
<br>
<br>
```{r, eval=F}
#--- open the netCDF ---#
nc <- nc_open("pr_2019.nc")

#--- get variable name and dates ---#
var.id <- names(nc$var)
date.vector <- as_date(nc$dim$day$vals,origin="1900-01-01")

#--- read in observations ---#
nc.data.v <- ncvar_get(nc = nc, varid = var.id)[,,]

#--- collapse to matrix with lat/lon on rows and dates as columns ---#
nc.data <- array(nc.data.v,dim=c(prod(dim(nc.data.v)[1:2]),dim(nc.data.v)[3])) %>%
  as_tibble(.name_repair = "universal") %>%
  rename_all(~str_c(date.vector))

#--- organize nc.data into dataframe ---#
nc.df <- bind_cols(slicecoords,nc.data) %>% 
  mutate(lon = round(lon,3),
         lat = round(lat,3)) %>% 
  dplyr::select(-cell)

#####

#--- prepare bridge ---#
bridge.ready <- bridge %>% 
  mutate(lon = round(lon,3),
         lat = round(lat,3))
```
<br>
With the netCDF data and the bridge ready, we can merge them and aggregate. We join the data to the bridge by the coordinates, and then move from a wide to a long dataframe. It was wide (columns were days), and we pivot it to long so that each row is a cell-day. Then, we create the value weighted of the weather in a gridMET cell as it contributes to the polygon weather, using the `total index` value from our bridge. This way, when we group by day and polygon to summarize, we sum our weighted values (`value`). For comparison, we also calculate the alternative, unweighted values. 
<br>
<br>
```{r, eval=F}
var.polygon <- left_join(bridge.ready,
                         nc.df,
                         by=c("lat","lon")) %>% 
  pivot_longer(-c(lon,lat,GEOID,totalindex),
               names_to = "date",
               values_to = "value") %>% 
  mutate(valueweighted = totalindex*value) %>% 
  group_by(GEOID,date) %>%
  summarize(check=sum(totalindex), 
            valuew=sum(valueweighted),
            alt=mean(value)) %>%
  ungroup() %>%
  mutate(date=ymd(date))
```
<br>
Depending on the year and variable, the difference between the weighted and unweighted estimates will vary. In this case, we see that weighting tends to lower the distribution of estimates, suggesting that not weighting will attribute more precipitation to the study area than is actually experienced. 
<br>
<br>
```{r echo=FALSE}
# projects/Pop Weights/

# tp <- var.polygon %>% 
#   rename(Weighted = "valuew",
#          Unweighted = "alt") %>% 
#   pivot_longer(c(Weighted,Unweighted)) %>% 
#   filter(value > 1, value < 50) 
# 
# write_rds(tp,"projects/Pop Weights/tp.rds")
# write_rds(var.polygon,"projects/Pop Weights/varpolygon.rds")
var.polygon <- read_rds("varpolygon.rds")
tp <- read_rds("tp.rds")

p <- ggplot(tp, aes(x = value, fill = name)) +
  geom_density(color = "grey", alpha = 0.4) +
  scale_fill_manual(values = c("#59344F","#F5E0B7")) +
  labs(title = "Distribution of Precip., 2018",
       subtitle = "In CBSA-Days with 1-50 mm",
       fill = "Estimates",
       x = "",
       y = "") +
  theme_tufte()

p
```
<br>
<br>
<br>

## Regression

<br>
Using the daily-county weather data we created and the Bureau of Transportation Statistics' [Fatality Analysis Reporting System](https://data-usdot.opendata.arcgis.com/datasets/fatal-motor-vehicle-accidents-person)'s dataset on fatal motor vehicle accidents, we can estimate the relationship between precipitation and fatal motor vehicle accidents.
<br>
<br>
```{r}
#--- reading in and preparing the regression-ready dataframe ---#
fars <- read_csv("Fatality_Analysis_Reporting_System.csv") %>% 
  dplyr::select(STATENAME,STATE,COUNTY,MONTH,DAY) %>% 
  mutate(GEOID = paste0(str_pad(STATE,2,pad = "0"),
                        str_pad(COUNTY,3,pad = "0")),
         date = ymd(paste0("2019-",MONTH,"-",DAY))) %>% 
  group_by(GEOID,date) %>% 
  summarise(FA = n()) %>% 
  ungroup() %>% 
  left_join(var.polygon,.,by = c("GEOID","date")) %>% 
  replace_na(list(FA=0)) %>% 
  mutate(FAany = ifelse(FA > 0,1,0))

f1 <- lm(FA ~ valuew, data = fars)
summary(f1)
```
<br>
We see that the average county on a given day experienced 0.063 accidents per day, and that precipitation alone does not cause accidents. We could improve this model by repeating the extraction steps with other possible gridMET weather variables that might predict car accidents like temperatures, wind, and particulate matter.


